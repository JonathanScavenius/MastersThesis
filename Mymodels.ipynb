{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages\n",
    "\n",
    "#!pip install import-ipynb\n",
    "#!pip install git+https://github.com/patrick-kidger/torchcubicspline.git # this pip install shouldn't be strictly needed but kept just in case.\n",
    "import time\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchcubicspline import(natural_cubic_spline_coeffs, \n",
    "                             NaturalCubicSpline)\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.optimize import newton,least_squares\n",
    "from torch.func import hessian\n",
    "import math\n",
    "from torch.func import jacfwd\n",
    "from torch.func import vmap, vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set default torch values to float64\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# set seed, to ensure same weight init across models\n",
    "def set_seed(inte):\n",
    "    torch.manual_seed(inte)\n",
    "\n",
    "\n",
    "# device setting for if Cuda is available, i.e. allows running on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used across all networks\n",
    "\n",
    "# Learing rate scheduler, decays learning rate every 50 epochs\n",
    "\n",
    "class CustomLRScheduler(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, last_epoch = -1):\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "    def get_lr(self):\n",
    "        lr_factor = 0.9**(self.last_epoch//50)\n",
    "        return [base_lr*lr_factor for base_lr in self.base_lrs]\n",
    "    \n",
    "# class for centered soft max\n",
    "\n",
    "class nn_centered_softmax(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return 1/(1+torch.exp(-input))- 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Torch functions for calculating swaps, zero coupon prices and interpolation\n",
    "\n",
    "\n",
    "# function for discounting zero coupon rates for tensors\n",
    "\n",
    "def discounting_torch(curve, maturity): \n",
    "    return torch.exp(-maturity * curve[int(maturity)-1]) \n",
    "\n",
    "\n",
    "# function for calculating swap rates for tensors, calls discounting_torch().\n",
    "\n",
    "def swap_rate_torch_paper(curve, maturities):\n",
    "    swap_rates = torch.empty(0) \n",
    "    for maturity in maturities:\n",
    "        float_leg = 1 - discounting_torch(curve, maturity)\n",
    "        fixed_leg = torch.sum(torch.stack([discounting_torch(curve, t) for t in range(1,int(maturity)+1)]))\n",
    "        swap_rate = float_leg / fixed_leg\n",
    "        swap_rates = torch.cat(( swap_rates,swap_rate.unsqueeze(0)))\n",
    "    return swap_rates\n",
    "\n",
    "\n",
    "# curve class for doing interpolation for tensors, currently not needed. \n",
    "class Curve_torch():\n",
    "    def __init__(self, x_values, y_values):\n",
    "\n",
    "        cs = natural_cubic_spline_coeffs(x_values, y_values.reshape(6,1))\n",
    "        spline = NaturalCubicSpline(cs)\n",
    "        self.x_values = np.linspace(0,30,30* 365, endpoint=True)\n",
    "        torch_x_values = torch.empty(0)\n",
    "        torch_y_values = torch.empty(0)\n",
    "        for i in self.x_values: \n",
    "            torch_y_values = torch.cat((torch_y_values,(spline.evaluate(torch.tensor(i)))))\n",
    "\n",
    "            torch_x_values = torch.cat((torch_x_values,(torch.tensor([i]))))\n",
    "        \n",
    "        self.y_values = torch_y_values\n",
    "        self.x_values = torch_x_values\n",
    "\n",
    "\n",
    "    def set_value(self, x, y):\n",
    "        idx = torch.abs(self.x_values - x).argmin()\n",
    "        self.y_values[idx] = y\n",
    "\n",
    "    def get_value(self, x):\n",
    "        # Find the index of the nearest x value\n",
    "        idx = torch.abs(self.x_values - x).argmin()\n",
    "        return self.y_values[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions used to calculate the arbitrage condition and needed compontents for this, ie sigma, mu and derivatives. Both 2-factor and 3-factor versions\n",
    "# exists for each function (if needed)\n",
    "\n",
    "\n",
    "# construction and calculation of gradients used in arbitrage condition\n",
    "def construct_grad_z2(taus, latents, model):\n",
    "    taus = torch.full((latents.size()[0], 1), taus)      \n",
    "\n",
    "    #compute jacobian\n",
    "    compute_batch_jacobian = vmap(jacfwd(model.decoder, argnums=0), in_dims=(0)) \n",
    "    latent_grad = compute_batch_jacobian(torch.cat((latents,taus),dim=1))\n",
    "\n",
    "    # Reshaping gradients\n",
    "    grad_z = latent_grad[:, :, :2]  \n",
    "    grad_tau = latent_grad[:, :, 2:]  \n",
    "    grad_tau = torch.squeeze(grad_tau, dim=2)\n",
    "    grad_z = torch.squeeze(grad_z, dim=2)\n",
    "    \n",
    "    return grad_tau, grad_z\n",
    "\n",
    "\n",
    "# construction and calculation of gradients used in arbitrage condition\n",
    "def construct_grad_z3(taus, latents, model):\n",
    "    taus = torch.full((latents.size()[0], 1), taus)      \n",
    "\n",
    "    #compute jacobian\n",
    "    compute_batch_jacobian = vmap(jacfwd(model.decoder, argnums=0), in_dims=(0)) \n",
    "    latent_grad = compute_batch_jacobian(torch.cat((latents,taus),dim=1))\n",
    "\n",
    "    # Reshaping gradients\n",
    "    grad_z = latent_grad[:, :, :3]  # Selects first 2 elements along the last dimension\n",
    "    grad_tau = latent_grad[:, :, 3:]  # Selects the last element along the last dimension\n",
    "    grad_tau = torch.squeeze(grad_tau, dim=2)\n",
    "    grad_z = torch.squeeze(grad_z, dim=2)\n",
    "    \n",
    "    return grad_tau, grad_z\n",
    "\n",
    "# construction and calculation of hessian used in arbitrage condition\n",
    "def construct_hessian(taus, latents, model):    \n",
    "    \n",
    "    taus = torch.full((latents.size()[0], 1), taus)          \n",
    "    # compute hessian\n",
    "    compute_batch_hessian = vmap(hessian(model.decoder, argnums=0), in_dims=(0))\n",
    "    hessian_z = compute_batch_hessian(torch.cat((latents,taus), dim=1)) \n",
    "\n",
    "    # reshaping hessian\n",
    "    removed_row = hessian_z[:, :, :-1, :]\n",
    "    hessian_z = removed_row[:, :, :, :-1]\n",
    "    hessian_z = torch.squeeze(hessian_z,dim=1)\n",
    "\n",
    "    return hessian_z\n",
    "\n",
    "\n",
    "# Function for constructing the sigma matrix given output of network\n",
    "def construct_sigma2(sigma1, sigma2, rho):\n",
    "    final_sigma = torch.rand(len(sigma1), 2, 2)\n",
    "    for i in range(len(sigma1)):\n",
    "        final_sigma[i, 0, 0] = sigma1[i]\n",
    "        final_sigma[i, 0, 1] = 0\n",
    "        final_sigma[i, 1, 0] = rho[i] * sigma2[i]\n",
    "        final_sigma[i, 1, 1] = torch.sqrt(1 - torch.pow(rho[i], 2)) * sigma2[i]\n",
    "    return final_sigma\n",
    "\n",
    "# Function for constructing the sigma matrix given output of network\n",
    "def construct_sigma3(sigma1, sigma2, sigma3, rho12, rho13, rho23):\n",
    "    final_sigma = torch.empty(len(sigma1), 3, 3)\n",
    "    for i in range(len(sigma1)):\n",
    "        final_sigma[i, 0, 0] = sigma1[i]\n",
    "        final_sigma[i, 0, 1] = 0\n",
    "        final_sigma[i, 0, 2] = 0\n",
    "        final_sigma[i, 1, 0] = rho12[i] * sigma2[i]\n",
    "        final_sigma[i, 1, 1] = torch.sqrt(1 - torch.pow(rho12[i], 2)) * sigma2[i]\n",
    "        final_sigma[i, 1, 2] = 0\n",
    "        final_sigma[i, 2, 0] = sigma3[i]*rho13[i]\n",
    "        final_sigma[i, 2, 1] = sigma3[i]*((rho23[i]-rho13[i]*rho12[i])/(torch.sqrt(1 - torch.pow(rho12[i], 2))))\n",
    "        final_sigma[i, 2, 2] = sigma3[i]*(torch.sqrt(1-(torch.pow(rho13[i], 2)-(torch.pow((rho23[i]-rho13[i]*rho12[i])/(torch.sqrt(1 - torch.pow(rho12[i], 2))), 2)))))\n",
    "    return final_sigma\n",
    "\n",
    "# Function for calculation of the arbitrage condition \n",
    "def arbitrage_condition2(pi, r, partial_tau, grad_z, mu, sigma_matrix, hessian_z): \n",
    "    L = torch.rand(len(pi),1)\n",
    "\n",
    "    for i in range(len(pi)):\n",
    "        first_part = (-r[i]*pi[i]) - partial_tau[i] + (grad_z[i] @ mu[i].unsqueeze(0).reshape(2,1))\n",
    "        second_part = (0.5*torch.trace(torch.transpose(sigma_matrix[i], 0,1) * hessian_z[i] * sigma_matrix[i]))\n",
    "\n",
    "        L[i] = first_part + second_part\n",
    "    return L\n",
    "\n",
    "\n",
    "# Function for calculation of the arbitrage condition \n",
    "def arbitrage_condition3(pi, r, partial_tau, grad_z, mu, sigma_matrix, hessian_z): \n",
    "    L = torch.rand(len(pi),1)\n",
    "\n",
    "    for i in range(len(pi)):\n",
    "        first_part = (-r[i]*pi[i]) - partial_tau[i] + (grad_z[i] @ mu[i].unsqueeze(0).reshape(3,1))\n",
    "        second_part = (0.5*torch.trace(torch.transpose(sigma_matrix[i], 0,1) * hessian_z[i] * sigma_matrix[i]))\n",
    "\n",
    "        L[i] = first_part + second_part\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss functions for networks\n",
    "\n",
    "\n",
    "# Custom loss function for adding the arbitrage condition with MSE\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, yhat, y, L, w):  \n",
    "        mse = torch.mean(torch.sum(torch.pow(yhat - y, 2),1)/8 )\n",
    "        arbitrage_loss = torch.mean(torch.sum(torch.pow(L, 2), 1)/30)\n",
    "       \n",
    "        return mse + (w*arbitrage_loss)\n",
    "    \n",
    "# Custom loss function, with added lipshitz regulization (not used, but was used during development)\n",
    "class CustomLipshitzLoss(nn.Module):\n",
    "    def __init__(self): \n",
    "            super().__init__()\n",
    "    def forward(self, yhat, y, L, w, lipshitz, beta):\n",
    "        mse = torch.mean(torch.sum(torch.pow((yhat-y), 2),1))\n",
    "        arbitrage_loss = torch.mean(torch.sum(torch.pow(L, 2),1)*w)     \n",
    "        return mse + arbitrage_loss + beta * lipshitz\n",
    "        \n",
    "# Custom loss function for adding the arbitrage condition with MSE and Kullback Leibler Divergence\n",
    "class CustomklLoss(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, yhat, y, L, w, mean, var): \n",
    "\n",
    "        mse = torch.mean(torch.sum(torch.pow(yhat - y, 2),1)/8 )\n",
    "        arbitrage_loss = torch.mean(torch.sum(torch.pow(L, 2), 1)/30 * w)\n",
    "        kl = - torch.mean(torch.sum(1 + torch.log(var.pow(2)) - mean.pow(2) - var.pow(2) ,1))\n",
    "       \n",
    "        return mse  + 1e-7*kl + arbitrage_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the 2-factor Financed Informed Auto-Encoder model. It should be noted that if one wishes to evaluate the model without the financed informed part,\n",
    "# one should remove the arbitrage_loss from the CustomLoss class(). While this isnt the most efficient way of doing this, it still works :-) and leaves less\n",
    "# code clutter.\n",
    "\n",
    "class FIRAutoEncoder(nn.Module):\n",
    "     def __init__(self, in_features, latent_dim):\n",
    "        super().__init__()\n",
    "        self.name = \"FIRAE\"\n",
    "        self.in_features = in_features\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.in_features,self.latent_dim, bias = False)\n",
    "            \n",
    "                            )\n",
    "        self.volatility = nn.Sequential(\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(self.latent_dim,3, bias = False),\n",
    "            nn.Linear(3,3, bias = False)\n",
    "        )\n",
    "        \n",
    "        self.drift = nn.Sequential(\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(self.latent_dim,2, bias = False),\n",
    "            nn.Linear(2,2,bias =False)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3,10, bias = False),\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(10,1, bias = False)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                \n",
    "\n",
    "     def forward(self,x):\n",
    "        # AE part of network\n",
    "        x, maturities = x[:-1][0], x[-1]\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = []\n",
    "        for tau in maturities: \n",
    "            taus = torch.full((x.size()[0], 1), tau)\n",
    "            decoded.append( self.decoder(torch.cat((encoded.requires_grad_(True),taus.requires_grad_(True)), dim=1)) )\n",
    "        \n",
    "        \n",
    "        # financial information part\n",
    "        mu = self.drift(encoded)\n",
    "        sigma_1, sigma_2, rho = torch.split(self.volatility(encoded),1,dim=1)\n",
    "        decode = torch.stack(decoded, dim=1)\n",
    "        \n",
    "        return decode.reshape(len(x), 31), encoded, mu, torch.exp(sigma_1), torch.exp(sigma_2), torch.tanh(rho) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for the 2-factor Auto-Encoder models, includes both at train step and validation step\n",
    "\n",
    "def train_ae(model, data, epochs, optimizer, criterion,maturities,swap_maturities, scheduler, valloader):\n",
    "    losses = []\n",
    "    outputs = []\n",
    "    acc_loss = 0\n",
    "    result_zcs = []\n",
    "    val_losses = []\n",
    "    val_acc_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch number:\",epoch)\n",
    "        for swaps in data:\n",
    "          if model.name == \"FIRAE\":\n",
    "            regulizer_curve = []\n",
    "            zc_rates, latents, mu, sigma1, sigma2, rho = model([swaps,maturities])\n",
    "            short_rate = zc_rates[:,0] \n",
    "            \n",
    "            # constructing sigma\n",
    "            sigma_matrix = construct_sigma2(sigma1, sigma2, rho) \n",
    "\n",
    "            # construct gradients and hessian from forward pass of decoder\n",
    "            maturities_1 = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
    "            for tau in maturities_1:\n",
    "              \n",
    "              # constructing gradients and hessian\n",
    "              grad_tau, grad_z = construct_grad_z2(tau, latents, model)\n",
    "              hessian_z = construct_hessian(tau, latents, model)\n",
    "\n",
    "              # arbitrage condition\n",
    "              regulizer = arbitrage_condition2(zc_rates[:,tau], short_rate, grad_tau, grad_z, mu, sigma_matrix, hessian_z)             \n",
    "              regulizer_curve.append(regulizer)\n",
    "              result_regulizer = torch.cat(regulizer_curve, dim=1)\n",
    "\n",
    "           \n",
    "\n",
    "            swaps_calc = []            \n",
    "            # calculate the swaps:\n",
    "            for i in range(len(zc_rates)):\n",
    "               swaps_calc.append(swap_rate_torch_paper(zc_rates[i], swap_maturities))\n",
    "              \n",
    "            result_swaps = torch.stack(swaps_calc, dim = 0)\n",
    "              \n",
    "            # loss part of the network\n",
    "            loss = criterion(result_swaps, swaps, result_regulizer, 5) \n",
    "            print(loss)\n",
    "            acc_loss += loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step() \n",
    "\n",
    "\n",
    "\n",
    "            # validation step, ie evaluate current model on validation set.\n",
    "        for val_swaps in valloader:\n",
    "            if model.name == \"FIRAE\":\n",
    "              model.eval()\n",
    "\n",
    "              \n",
    "              val_regulizer_curve = []\n",
    "\n",
    "              val_zc_rates, val_latents, val_mu, val_sigma1, val_sigma2, val_rho = model([val_swaps,maturities])\n",
    "              val_short_rate = val_zc_rates[:,0]\n",
    "\n",
    "              val_sigma_matrix = construct_sigma2(val_sigma1, val_sigma2, val_rho)\n",
    "\n",
    "              for val_tau in maturities_1: \n",
    "                 val_grad_tau, val_grad_z = construct_grad_z2(val_tau, val_latents, model)\n",
    "                 val_hessian_z = construct_hessian(val_tau, val_latents, model)\n",
    "\n",
    "                 val_regulizer = arbitrage_condition2(val_zc_rates[:,val_tau], val_short_rate, val_grad_tau, val_grad_z, val_mu, val_sigma_matrix, val_hessian_z)\n",
    "                 val_regulizer_curve.append(val_regulizer)\n",
    "                \n",
    "                 val_result_regulizer = torch.cat(val_regulizer_curve, dim=1)\n",
    "            \n",
    "\n",
    "              val_swaps_calc = []\n",
    "              for i in range(len(val_zc_rates)):\n",
    "                  val_swaps_calc.append(swap_rate_torch_paper(val_zc_rates[i], swap_maturities))\n",
    "                  \n",
    "              val_result_swaps = torch.stack(val_swaps_calc, dim = 0)\n",
    "                \n",
    "                \n",
    "              val_loss = criterion(val_result_swaps, val_swaps, val_result_regulizer, 5) \n",
    "              val_acc_loss += val_loss\n",
    "              print(\"val loss:\" , val_acc_loss)\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        print(\"acc loss:\",acc_loss/40)\n",
    "        acc_loss = 0\n",
    "        val_acc_loss =0 \n",
    "        losses.append(loss.detach().numpy())\n",
    "        val_losses.append(val_loss.detach().numpy())\n",
    "        outputs.append((epochs, swaps.detach().numpy(), result_swaps.detach().numpy()))\n",
    "        result_zcs.append(zc_rates.detach().numpy())\n",
    "        \n",
    " \n",
    "    return model, losses, outputs,val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the 3-factor Financed Informed Auto-Encoder model. It should be noted that if one wishes to evaluate the model without the financed informed part,\n",
    "# one should remove the arbitrage_loss from the CustomLoss class(). While this isnt the most efficient way of doing this, it still works :-) and leaves less\n",
    "# code clutter.\n",
    "\n",
    "class FI3AutoEncoder(nn.Module):\n",
    "     def __init__(self, in_features, latent_dim):\n",
    "        super().__init__()\n",
    "        self.name = \"FI3AE\"\n",
    "        self.in_features = in_features\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.in_features,self.latent_dim, bias = False)\n",
    "                            )\n",
    "\n",
    "\n",
    "        self.volatility = nn.Sequential(\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(self.latent_dim,6, bias = False),\n",
    "            nn.Linear(6,6, bias = False)\n",
    "        )\n",
    "        \n",
    "        self.drift = nn.Sequential(\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(self.latent_dim,3, bias = False),\n",
    "            nn.Linear(3,3, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(            \n",
    "            nn.Linear(4,10, bias=False),\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(10,1,bias=False)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                \n",
    "\n",
    "     def forward(self,x):\n",
    "        # AE part of network\n",
    "        x, maturities = x[:-1][0], x[-1]\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = []\n",
    "\n",
    "        for tau in maturities:\n",
    "            taus = torch.full((x.size()[0], 1), tau)\n",
    "            decoded.append(self.decoder(torch.cat((encoded.requires_grad_(True),taus.requires_grad_(True)), dim=1))) \n",
    "        \n",
    "        # financial information part\n",
    "        mu = self.drift(encoded)\n",
    "        sigma_1, sigma_2, sigma_3, rho12, rho13, rho23 = torch.split(self.volatility(encoded),1,dim=1)\n",
    "       \n",
    "        decode = torch.stack(decoded,dim=1)\n",
    "\n",
    "        return decode.reshape(len(x),31), encoded, mu, torch.exp(sigma_1), torch.exp(sigma_2), torch.exp(sigma_3), torch.tanh(rho12), torch.tanh(rho13), torch.tanh(rho23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for the 3-factor Auto-Encoder models, includes both at train step and validation step\n",
    "\n",
    "def train_ae3(model, data, epochs, optimizer, criterion,maturities,swap_maturities, scheduler, valloader):\n",
    "    losses = []\n",
    "    outputs = []\n",
    "    acc_loss = 0\n",
    "    result_zcs = []\n",
    "    val_losses = []\n",
    "    val_acc_loss = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch number:\",epoch)\n",
    "        for swaps in data:\n",
    "          if model.name == \"FI3AE\":\n",
    "          \n",
    "            regulizer_curve = []\n",
    "\n",
    "            zc_rates, latents, mu, sigma1, sigma2, sigma3, rho12, rho13, rho23 = model([swaps,maturities])  \n",
    "            \n",
    "            short_rate = zc_rates[:,0]\n",
    "\n",
    "            # constructing sigma\n",
    "            sigma_matrix = construct_sigma3(sigma1, sigma2, sigma3, rho12, rho13, rho23)\n",
    "            maturities_1 = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
    "            for tau in maturities_1:\n",
    "              # construct gradients and hessian from forward pass of decoder\n",
    "              grad_tau, grad_z = construct_grad_z3(tau, latents, model)\n",
    "              hessian_z = construct_hessian(tau, latents, model)\n",
    "\n",
    "              # arbitrage condition\n",
    "              regulizer = arbitrage_condition3(zc_rates[:,tau], short_rate, grad_tau, grad_z, mu, sigma_matrix, hessian_z)\n",
    "              regulizer_curve.append(regulizer)\n",
    "\n",
    "              result_regulizer = torch.cat(regulizer_curve, dim=1)\n",
    "\n",
    "          \n",
    "            swaps_calc = []\n",
    "            # calculate the swaps:\n",
    "            for i in range(len(zc_rates)):\n",
    "               swaps_calc.append(swap_rate_torch_paper(zc_rates[i], swap_maturities))\n",
    "              \n",
    "            result_swaps = torch.stack(swaps_calc, dim = 0).requires_grad_(True)\n",
    "              \n",
    "            # loss \n",
    "            loss = criterion(result_swaps, swaps, result_regulizer, 5) \n",
    "            print(loss)\n",
    "            acc_loss += loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step() \n",
    "            \n",
    "        # Validation step    \n",
    "        for val_swaps in valloader:\n",
    "          if model.name == \"FI3AE\":\n",
    "            model.eval()\n",
    "   \n",
    "            val_regulizer_curve = []\n",
    "\n",
    "            val_zc_rates, val_latents, val_mu, val_sigma1, val_sigma2, val_sigma3,val_rho12,val_rho13,val_rho23 = model([val_swaps,maturities])\n",
    "            val_short_rate = val_zc_rates[:,0]\n",
    "\n",
    "            val_sigma_matrix = construct_sigma3(val_sigma1, val_sigma2, val_sigma3, val_rho12,val_rho13,val_rho23)\n",
    "\n",
    "            for val_tau in maturities_1:\n",
    "              val_grad_tau, val_grad_z = construct_grad_z3(val_tau, val_latents, model)\n",
    "              val_hessian_z = construct_hessian(val_tau, val_latents, model)\n",
    "\n",
    "              val_regulizer = arbitrage_condition3(val_zc_rates[:,val_tau], val_short_rate, val_grad_tau, val_grad_z, val_mu, val_sigma_matrix, val_hessian_z)\n",
    "              val_regulizer_curve.append(val_regulizer)\n",
    "        \n",
    "              val_result_regulizer = torch.cat(val_regulizer_curve, dim=1)\n",
    "\n",
    "            val_swaps_calc = []\n",
    "            # calculate the swaps:\n",
    "            for i in range(len(val_zc_rates)):\n",
    "              val_swaps_calc.append(swap_rate_torch_paper(val_zc_rates[i], swap_maturities))\n",
    "                \n",
    "            val_result_swaps = torch.stack(val_swaps_calc, dim = 0)\n",
    "                            \n",
    "            val_loss = criterion(val_result_swaps, val_swaps, val_result_regulizer, 5) \n",
    "            val_acc_loss += val_loss\n",
    "            print(\"val loss:\" , val_acc_loss)\n",
    "  \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        print(\"acc loss:\",acc_loss/40)\n",
    "        acc_loss = 0\n",
    "        val_acc_loss =0 \n",
    "        losses.append(loss.detach().numpy())\n",
    "        val_losses.append(val_loss.detach().numpy())\n",
    "        outputs.append((epochs, swaps.detach().numpy(), result_swaps.detach().numpy()))\n",
    "        result_zcs.append(zc_rates.detach().numpy())\n",
    "        \n",
    " \n",
    "    return model, losses, outputs,val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the 2-factor Financed Informed Variational Auto-Encoder model. It should be noted that if one wishes to evaluate the model without the financed informed\n",
    "# part, one should remove the arbitrage_loss from the CustomLoss class(). While this isnt the most efficient way of doing this, it still works :-) and leaves less\n",
    "# code clutter.\n",
    "\n",
    "class FirVariationalAutoEncoder(nn.Module): \n",
    "    def __init__(self, in_features, latent_dim): \n",
    "        super().__init__()\n",
    "        self.name = \"FIRVAE\"\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.in_features,5, bias = False),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(5,5, bias = False),\n",
    "        )\n",
    "\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Linear(5,self.latent_dim, bias = False)            \n",
    "                            )\n",
    "        \n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Linear(5,self.latent_dim, bias = False)            \n",
    "\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3,10, bias = False),\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(10,1, bias = False)\n",
    "                            )\n",
    "        \n",
    "        self.volatility = nn.Sequential(\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(self.latent_dim,3, bias = False),\n",
    "            nn.Linear(3,3, bias = False)\n",
    "        )\n",
    "        \n",
    "        self.drift = nn.Sequential(\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(self.latent_dim,2, bias = False),\n",
    "            nn.Linear(2,2, bias = False)\n",
    "        )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        # VAE part of the network        \n",
    "        x, maturities = x[:-1][0], x[-1]\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        mean1, var1 = torch.split(self.encoder1(x), 1, dim=1)\n",
    "        mean2, var2 = torch.split(self.encoder2(x),1, dim=1)\n",
    "        \n",
    "        N = torch.distributions.Normal(0, 1)\n",
    "\n",
    "        var1 = torch.exp(var1)\n",
    "        var2 = torch.exp(var2)\n",
    "        \n",
    "        z1 = mean1 + var1*N.sample(mean1.shape)\n",
    "        z2 = mean2 + var2*N.sample(mean2.shape)\n",
    "\n",
    "        z = torch.cat((z1,z2), dim=1)\n",
    "\n",
    "        mean = torch.cat((mean1, mean2), dim = 1)\n",
    "\n",
    "        var = torch.cat((var1, var2), dim = 1)\n",
    "        decoded = []\n",
    "        for tau in maturities:\n",
    "            taus = torch.full((x.size()[0], 1), tau)\n",
    "            decoded.append(self.decoder(torch.cat((z.requires_grad_(True),taus), dim=1)))\n",
    "        \n",
    "\n",
    "        # finacial information part\n",
    "        mu = self.drift(z)\n",
    "        sigma_1, sigma_2, rho = torch.split(self.volatility(z),1,dim=1)\n",
    "        decode = torch.stack(decoded, dim=1)\n",
    "        return decode.reshape(len(x),31), z , mu, torch.exp(sigma_1), torch.exp(sigma_2),torch.tanh(rho), var, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for the 2-factor Variational Auto-Encoder models, includes both at train step and validation step\n",
    "\n",
    "\n",
    "def train_vae(model, data, epochs, optimizer, criterion,maturities,swap_maturities, scheduler,valloader):\n",
    "    losses = []\n",
    "    outputs = []\n",
    "    val_losses = []\n",
    "    acc_loss = 0\n",
    "    val_acc_loss = 0\n",
    "    result_zcs = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch number:\",epoch)\n",
    "        for swaps in data:\n",
    "          if model.name == \"FIRVAE\":\n",
    "            regulizer_curve = []\n",
    "            zc_rates, latents, mu, sigma1, sigma2, rho,var,mean =  model([swaps, maturities])\n",
    "            short_rate = zc_rates[:,0]\n",
    "       \n",
    "            # constructing sigma matrix\n",
    "            sigma_matrix = construct_sigma2(sigma1, sigma2, rho)\n",
    "\n",
    "            maturities_1 = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
    "            for tau in maturities_1: \n",
    "\n",
    "                # construct gradients and hessian from forward pass of decoder\n",
    "                grad_tau, grad_z = construct_grad_z2(tau, latents, model)\n",
    "                hessian_z = construct_hessian(tau, latents, model)\n",
    "            \n",
    "                # arbitrage condition\n",
    "                regulizer = arbitrage_condition2(zc_rates[:,tau], short_rate, grad_tau, grad_z, mu, sigma_matrix, hessian_z)\n",
    "                regulizer_curve.append(regulizer)\n",
    "\n",
    "                result_regulizer = torch.cat(regulizer_curve, dim=1)\n",
    "  \n",
    "\n",
    "            swaps_calc = []\n",
    "            # calculate the swaps:\n",
    "            for i in range(len(zc_rates)):\n",
    "               swaps_calc.append(swap_rate_torch_paper(zc_rates[i], swap_maturities))\n",
    "              \n",
    "            result_swaps = torch.stack(swaps_calc, dim = 0)\n",
    "\n",
    "            # Calculating the loss function     \n",
    "            loss = criterion(result_swaps, swaps, result_regulizer, 1, mean, var)  \n",
    "            print(loss)\n",
    "            acc_loss += loss\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        # Validation step\n",
    "        for val_swaps in valloader: \n",
    "            if model.name == \"FIRVAE\":\n",
    "              model.eval()\n",
    "              val_regulizer_curve = []\n",
    "              val_zc_rates, val_latents, val_mu, val_sigma1, val_sigma2, val_rho,val_var,val_mean = model([val_swaps,maturities])\n",
    "              \n",
    "              val_short_rate = val_zc_rates[:,0]\n",
    "              val_sigma_matrix = construct_sigma2(val_sigma1, val_sigma2, val_rho)\n",
    "\n",
    "              for val_tau in maturities_1: \n",
    "                 val_grad_tau, val_grad_z = construct_grad_z2(val_tau, val_latents, model)\n",
    "                 val_hessian_z = construct_hessian(val_tau, val_latents, model)\n",
    "\n",
    "                 val_regulizer = arbitrage_condition2(val_zc_rates[:,val_tau], val_short_rate, val_grad_tau, val_grad_z, val_mu, val_sigma_matrix, val_hessian_z)\n",
    "                 val_regulizer_curve.append(val_regulizer)\n",
    "        \n",
    "                 val_result_regulizer = torch.cat(val_regulizer_curve, dim=1)\n",
    "    \n",
    "              val_swaps_calc = []\n",
    "              # calculate the swaps:\n",
    "              for i in range(len(val_zc_rates)):\n",
    "                val_swaps_calc.append(swap_rate_torch_paper(val_zc_rates[i], swap_maturities))\n",
    "                  \n",
    "              val_result_swaps = torch.stack(val_swaps_calc, dim = 0)\n",
    "\n",
    "              val_loss = criterion(val_result_swaps, val_swaps, val_result_regulizer, 1,val_mean,val_var) \n",
    "              val_acc_loss += val_loss\n",
    "              print(\"val loss:\" , val_acc_loss)\n",
    "          \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        print(\"acc loss:\",acc_loss/40)\n",
    "        acc_loss = 0\n",
    "        val_acc_loss = 0\n",
    "        losses.append(loss.detach().numpy())\n",
    "        val_losses.append(val_loss.detach().numpy())\n",
    "        outputs.append((epochs, swaps.detach().numpy(), result_swaps.detach().numpy()))\n",
    "        result_zcs.append(zc_rates.detach().numpy())\n",
    "        \n",
    " \n",
    "    return model, losses, outputs, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the 3-factor Financed Informed Variational Auto-Encoder model. It should be noted that if one wishes to evaluate the model without the financed informed\n",
    "# part, one should remove the arbitrage_loss from the CustomLoss class(). While this isnt the most efficient way of doing this, it still works :-) and leaves less\n",
    "# code clutter.\n",
    "\n",
    "\n",
    "class Fi3VariationalAutoEncoder(nn.Module): \n",
    "    def __init__(self, in_features, latent_dim): \n",
    "        super().__init__()\n",
    "        self.name = \"FI3VAE\"\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.in_features, 5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(5,5)\n",
    "        )\n",
    "\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Linear(5,2, bias = False)            \n",
    "                            )\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Linear(5,2, bias = False)            \n",
    "                            )\n",
    "        \n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Linear(5,2, bias = False)            \n",
    "                            )\n",
    "    \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(4,10, bias = False),\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(10,1, bias = False)\n",
    "                            )\n",
    "        \n",
    "        self.volatility = nn.Sequential(\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(self.latent_dim,6, bias = False),\n",
    "            nn.Linear(6,6, bias = False)\n",
    "        )\n",
    "        \n",
    "        self.drift = nn.Sequential(\n",
    "            nn_centered_softmax(),\n",
    "            nn.Linear(self.latent_dim,3, bias = False),\n",
    "            nn.Linear(3,3)\n",
    "        )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self,x):\n",
    "        # VAE part of the network        \n",
    "\n",
    "        x, maturities = x[:-1][0], x[-1]\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        mean1, var1 = torch.split(self.encoder1(x), 1, dim=1)\n",
    "        mean2, var2 = torch.split(self.encoder2(x), 1, dim=1)\n",
    "        mean3, var3 = torch.split(self.encoder3(x), 1, dim=1)\n",
    "       \n",
    "\n",
    "        var1 = torch.exp(var1)\n",
    "        var2 = torch.exp(var2)\n",
    "        var3 = torch.exp(var3)\n",
    "\n",
    "        N = torch.distributions.Normal(0, 1)\n",
    "\n",
    "        \n",
    "        z1 = mean1 + var1*N.sample(mean1.shape)\n",
    "        z2 = mean2 + var2*N.sample(mean2.shape)\n",
    "        z3 = mean3 + var3*N.sample(mean3.shape)\n",
    "\n",
    "        z = torch.cat((z1,z2, z3), dim=1)\n",
    "        mean = torch.cat((mean1, mean2, mean3), dim = 1)\n",
    "        var = torch.cat((var1, var2, var3), dim = 1)\n",
    "\n",
    "        decoded = []\n",
    "        for tau in maturities: \n",
    "            taus = torch.full((x.size()[0], 1), tau)\n",
    "            decoded.append(self.decoder(torch.cat((z.requires_grad_(True),taus), dim=1)))\n",
    "            \n",
    "\n",
    "        # finacial information part\n",
    "        mu = self.drift(z)\n",
    "        sigma_1, sigma_2, sigma_3, rho12, rho13, rho23 = torch.split(self.volatility(z),1,dim=1)\n",
    "        decode = torch.stack(decoded, dim=1)\n",
    "        \n",
    "        return decode.reshape(len(x), 31), z , mu, torch.exp(sigma_1), torch.exp(sigma_2), torch.exp(sigma_3), torch.tanh(rho12), torch.tanh(rho13), torch.tanh(rho23), var, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for the 3-factor Variational Auto-Encoder models, includes both at train step and validation step\n",
    "\n",
    "\n",
    "def train_vae3(model,data,epochs, optimizer, criterion, maturities, swap_maturities, scheduler, valloader):\n",
    "    losses = []\n",
    "    outputs = []\n",
    "    val_losses = []\n",
    "    acc_loss = 0\n",
    "    val_acc_loss = 0\n",
    "    result_zcs = []\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch number:\",epoch)\n",
    "        for swaps in data:\n",
    "          if model.name == \"FI3VAE\":\n",
    "            regulizer_curve = []\n",
    "            zc_rates, latents, mu, sigma1, sigma2, sigma3, rho12, rho13, rho23,var,mean = model([swaps,maturities])  \n",
    "            short_rate = zc_rates[:,0]\n",
    "\n",
    "            # constructing sigma\n",
    "            sigma_matrix = construct_sigma3(sigma1, sigma2, sigma3, rho12, rho13,rho23)\n",
    "            \n",
    "            maturities_1 = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30]\n",
    "            for tau in maturities_1:\n",
    "              \n",
    "              # construct gradients and hessian from forward pass of decoder\n",
    "              grad_tau, grad_z = construct_grad_z3(tau, latents, model)\n",
    "              hessian_z = construct_hessian(tau, latents, model)\n",
    "\n",
    "              # arbitrage condition\n",
    "              regulizer = arbitrage_condition3(zc_rates[:,tau], short_rate, grad_tau, grad_z, mu, sigma_matrix, hessian_z)\n",
    "              regulizer_curve.append(regulizer)\n",
    "\n",
    "              result_regulizer = torch.cat(regulizer_curve, dim=1)\n",
    "\n",
    "           \n",
    "            swaps_calc = []\n",
    "            # calculate the swaps:\n",
    "            for i in range(len(zc_rates)):\n",
    "               swaps_calc.append(swap_rate_torch_paper(zc_rates[i], swap_maturities))\n",
    "              \n",
    "            result_swaps = torch.stack(swaps_calc, dim = 0)\n",
    "              \n",
    "            # loss \n",
    "            loss = criterion(result_swaps, swaps, result_regulizer, 1, mean, var) \n",
    "            print(loss) \n",
    "            acc_loss += loss\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        for val_swaps in valloader:\n",
    "          if model.name == \"FI3VAE\":\n",
    "            model.eval()\n",
    "            val_regulizer_curve = []\n",
    "            val_zc_rates, val_latents, val_mu, val_sigma1, val_sigma2, val_sigma3,val_rho12,val_rho13,val_rho23, val_var, val_mean = model([val_swaps,maturities])\n",
    "            \n",
    "            val_short_rate = val_zc_rates[:,0]\n",
    "            val_sigma_matrix = construct_sigma3(val_sigma1, val_sigma2, val_sigma3, val_rho12,val_rho13,val_rho23)\n",
    "            \n",
    "            for val_tau in maturities_1:\n",
    "               \n",
    "              val_grad_tau, val_grad_z = construct_grad_z3(val_tau, val_latents, model)\n",
    "              val_hessian_z = construct_hessian(val_tau, val_latents, model)\n",
    "\n",
    "              val_regulizer = arbitrage_condition3(val_zc_rates[:, val_tau], val_short_rate, val_grad_tau, val_grad_z, val_mu, val_sigma_matrix, val_hessian_z)\n",
    "              val_regulizer_curve.append(val_regulizer)\n",
    "               \n",
    "              val_result_regulizer = torch.cat(val_regulizer_curve, dim=1)\n",
    "    \n",
    "\n",
    "\n",
    "            val_swaps_calc = []\n",
    "            for i in range(len(val_zc_rates)):\n",
    "              val_swaps_calc.append(swap_rate_torch_paper(val_zc_rates[i], swap_maturities))\n",
    "                \n",
    "            val_result_swaps = torch.stack(val_swaps_calc, dim = 0)\n",
    "              \n",
    "              \n",
    "            val_loss = criterion(val_result_swaps, val_swaps, val_result_regulizer, 1,val_mean, val_var)\n",
    "            val_acc_loss += val_loss\n",
    "            print(\"val loss:\" , val_acc_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        print(\"acc loss:\",acc_loss/40)\n",
    "        acc_loss = 0\n",
    "        val_acc_loss =0 \n",
    "        losses.append(loss.detach().numpy())\n",
    "        val_losses.append(val_loss.detach().numpy())\n",
    "        outputs.append((epochs, swaps.detach().numpy(), result_swaps.detach().numpy()))\n",
    "        result_zcs.append(zc_rates.detach().numpy())\n",
    " \n",
    "    return model, losses, outputs, val_losses"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
